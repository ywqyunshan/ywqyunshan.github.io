<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="FQ Blog">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://ywqyunshan.github.io//img/home-bg-jeep.jpg">
    <meta property="twitter:image" content="https://ywqyunshan.github.io//img/home-bg-jeep.jpg" />
    

    
    <meta name="title" content="PC和Android使用MLC-LLM部署中小模型" />
    <meta property="og:title" content="PC和Android使用MLC-LLM部署中小模型" />
    <meta property="twitter:title" content="PC和Android使用MLC-LLM部署中小模型" />
    

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    <meta property="twitter:description" content="" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="iigeoywq, button, 博客, 个人网站, 图形学, GIS, 座舱, 车载, Andorid, 系统">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>PC和Android使用MLC-LLM部署中小模型 | </title>

    <link rel="canonical" href="/2024/12/21/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link href="https://cdn.jsdelivr.net/gh/FortAwesome/Font-Awesome@5.15.1/css/all.css" rel="stylesheet" type="text/css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">FQ Blog</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                        <li>
                            <a href="/categories/tech">tech</a>
                        </li>
                        
                    
                    
		    
                        <li><a href="/archive/">ARCHIVE</a></li>
                    
                        <li><a href="/notes/">NOTES</a></li>
                    
                        <li><a href="/about/">ABOUT</a></li>
                    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/img/home-bg-jeep.jpg')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98" title="工程实战">
                            工程实战
                        </a>
                        
                    </div>
                    <h1>PC和Android使用MLC-LLM部署中小模型</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                    &#34;iigeoywq button&#34;
                             
                            on 
                            Saturday, December 21, 2024
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h2 id="一-中小模型介绍">一 中小模型介绍</h2>
<p>在https://huggingface.co/网站上先找了基础规模小于等于5B(十亿)参数的中小模型。</p>
<table>
<thead>
<tr>
<th align="left">模型名称</th>
<th align="left">所属机构</th>
<th align="left">基础规模</th>
<th align="left">原生格式</th>
<th align="left">量化格式</th>
<th align="left">部署格式</th>
<th align="left">主要用途</th>
<th align="left">模型地址连接</th>
<th align="left">特点说明</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Phi-3-mini</td>
<td align="left">Microsoft</td>
<td align="left">3.8B</td>
<td align="left">PyTorch/safetensors</td>
<td align="left">GPTQ(4bit/8bit), AWQ(4bit), exllama2</td>
<td align="left">ONNX, TensorRT, MLC-LLM</td>
<td align="left">代码生成,通用推理</td>
<td align="left"><a href="https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3">https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3</a></td>
<td align="left">高性能代码能力,低资源消耗</td>
</tr>
<tr>
<td align="left">internlm2_5-1_8b-chat</td>
<td align="left">上海AI实验室</td>
<td align="left">1.8B</td>
<td align="left">PyTorch/safetensors</td>
<td align="left">GPTQ(4bit/8bit), AWQ(4bit), W8A8</td>
<td align="left">ONNX, TensorRT, vLLM, MLC-LLM</td>
<td align="left">中文对话,知识问答</td>
<td align="left"><a href="https://huggingface.co/internlm/internlm2_5-1_8b-chat">https://huggingface.co/internlm/internlm2_5-1_8b-chat</a> <a href="https://github.com/InternLM/InternLM/blob/main/README_zh-CN.md">https://github.com/InternLM/InternLM/blob/main/README_zh-CN.md</a></td>
<td align="left">中文理解深入,上下文长度32K</td>
</tr>
<tr>
<td align="left">Qwen2.5</td>
<td align="left">阿里云</td>
<td align="left">3B/1.5B/0.5B</td>
<td align="left">PyTorch/safetensors</td>
<td align="left">GPTQ(3/4/8bit), AWQ(4bit), SqueezeLLM</td>
<td align="left">ONNX, TensorRT, vLLM, MLC-LLM, FastTransformer</td>
<td align="left">双语对话,代码生成</td>
<td align="left"><a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct">https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct</a> <a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct">https://huggingface.co/Qwen/Qwen2.5-3B-Instruct</a> <a href="https://qwen.readthedocs.io/zh-cn/latest/index.html">https://qwen.readthedocs.io/zh-cn/latest/index.html</a></td>
<td align="left">中英双语优秀,工具调用能力强</td>
</tr>
<tr>
<td align="left">SmolLM2</td>
<td align="left">MosaicML</td>
<td align="left">135/360M/1.7B</td>
<td align="left">PyTorch/safetensors</td>
<td align="left">GPTQ(3/4/8bit), AWQ(4bit)</td>
<td align="left">ONNX, TensorRT, MLC-LLM</td>
<td align="left">轻量级推理</td>
<td align="left"><a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9">https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9</a></td>
<td align="left">超轻量高效,适合边缘设备</td>
</tr>
<tr>
<td align="left">Gemma-2</td>
<td align="left">Google</td>
<td align="left">2B/7B</td>
<td align="left">PyTorch/safetensors/JAX</td>
<td align="left">GPTQ(4bit), AWQ(4bit), GGUF</td>
<td align="left">JAX, ONNX, vLLM, MLC-LLM</td>
<td align="left">通用推理,教育应用</td>
<td align="left"><a href="https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315">https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315</a></td>
<td align="left">开放许可证,文档完善</td>
</tr>
<tr>
<td align="left">Llama-3.2</td>
<td align="left">Meta</td>
<td align="left">1B/3B</td>
<td align="left">PyTorch/safetensors</td>
<td align="left">GPTQ(4bit/8bit), AWQ(4bit)</td>
<td align="left">ONNX, TensorRT, vLLM, MLC-LLM</td>
<td align="left">通用对话,多语言任务</td>
<td align="left"><a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a> <a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a> <a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2">https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2</a></td>
<td align="left">多语言支持,推理增强</td>
</tr>
</tbody>
</table>
<ul>
<li>基础规模 (Base Model Size)
<ul>
<li>小型模型(1B以下): SmolLM2的135M和360M版本,适合资源受限场景</li>
<li>中型模型(1-3B): Llama-3.2的1B/3B, Qwen2.5的1.5B/3B, internlm2.5的1.8B, Gemma-2的2B</li>
<li>较大模型(3B以上): Phi-3-mini的3.8B, Gemma-2的7B版本</li>
<li>不同规模适合不同应用场景,较小模型适合边缘部署,较大模型性能更好</li>
</ul>
</li>
<li>原生格式 (Native Format)
<ul>
<li>safetensors: 主流格式,具有更好的安全性和加载速度</li>
<li>PyTorch格式: 兼容性好,生态完善</li>
<li>JAX格式: Google专有,性能优化</li>
<li>多格式支持: 如Gemma-2同时支持多种格式,增加了灵活性</li>
</ul>
</li>
<li>量化格式 (Quantization)
<ul>
<li>GPTQ: 最通用的量化方案,支持3/4/8-bit精度</li>
<li>AWQ: 新一代4-bit量化方案,精度损失更小</li>
<li>专有格式: 如Qwen2.5的SqueezeLLM, internlm的W8A8等</li>
<li>exllama2: 针对消费级显卡优化的量化方案</li>
</ul>
</li>
<li>部署格式 (Deployment)
<ul>
<li>ONNX: 跨平台标准格式,生态完善</li>
<li>TensorRT: NVIDIA GPU加速方案</li>
<li>vLLM: 高性能服务部署方案</li>
<li>MLC-LLM: 面向多种硬件的统一部署方案</li>
<li>专有方案: 如FastTransformer(高性能推理),JAX(Google专用)等</li>
</ul>
</li>
</ul>
<h2 id="二-mlc-llm部署框架介绍">二 MLC-LLM部署框架介绍</h2>
<p>直接看官网<a href="https://llm.mlc.ai/">MLC LLM: Universal LLM Deployment Engine With ML Compilation</a>介绍</p>
<blockquote>
<p>MLC LLM(Machine Learning Compiler LLM)是一个开源的统一大语言模型部署引擎，它主要由机器学习编译器（TVM）和高性能部署引擎(MLCEngine)组成，该项目的使命是让每个人都能在各种平台上原生地开发、优化和部署LLM模型。</p>
</blockquote>
<p>
  <img src="/img/MLC_LLM%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b.png" alt="MLC_LLM工作流程">

</p>
<h2 id="三-环境准备">三 环境准备</h2>
<h3 id="31-ubunut系统">3.1 Ubunut系统</h3>
<h3 id="32-conda-虚拟软件包管理环境">3.2 conda （虚拟软件包管理环境）</h3>
<p>使用conda可以和本地的软件包独立，尤其python的各种依赖容易冲突</p>
<pre><code># 下载安装脚本，需要科学上网
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

## 建议使用国内镜像地址
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh

## 运行安装
chmod 777 Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
## 使环境变量生效
source ~/.bashrc

## 注意安装后会自动生成一个base默认环境，可以退出base环境，并设置永久不激活
# 关闭自动激活base
conda config --set auto_activate_base false
# 如果后续要开启自动激活
conda config --set auto_activate_base true
conda deactivate

## 查看conda 多个配置文件层级：
1. 系统级配置文件 (通常在 conda 安装目录下的 .condarc)
2. 用户级配置文件 (~/.condarc)

conda config --show-sources

conda config --set show_channel_urls yes

## 配置国内镜像 参考https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/

conda config --set show_channel_urls yes

</code></pre><h3 id="33-rust本地环境安装仅android部署需要">3.3 Rust（本地环境安装）仅Android部署需要</h3>
<p>编译mlc-llm源码以及Android so库的时候需要，pc部署不需要</p>
<pre><code> curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs &gt; rustup-init.sh

 chmod +x rustup-init.sh

 /rustup-init.sh 

 source $HOME/.cargo/env

 ## 查看rustup rustc cargo是否安装成功
 rustup --version
 rustc --version
 cargo --version
</code></pre><h3 id="34-ndk本地环境-仅android部署需要">3.4 NDK（本地环境） 仅Android部署需要</h3>
<p>编译android so库需要，pc部署不需要
ndk下载参考https://developer.android.com/studio?hl=zh-cn</p>
<p>配置NDK和MCL_HOME环境变量</p>
<pre><code>vi ~/.bashrc 
## 输入
export ANDROID_NDK=/home/win/Android/Sdk/ndk/27.0.11718014
export TVM_NDK_CC=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
export TVM_SOURCE_DIR=/home/win/llmworkspace/mlc-llm/3rdparty/tvm
export MLC_LLM_SOURCE_DIR=/home/win/llmworkspace/mlc-llm

source ~/.bashrc
</code></pre><h2 id="四-下载和转换模型">四 下载和转换模型</h2>
<h3 id="41-创建conada-mlc-转换模型环境-conda-mlc_pre环境">4.1 创建conada mlc 转换模型环境 （conda mlc_pre环境）</h3>
<pre><code># 创建conada mlc 转换模型环境
conda create -n mlc_pre python=3.11 (官方建议mlc依赖的python版本3.11)
# 进入conda环境
conda activate mlc_pre

conda install numpy=1.25.0 # 根据python版本决定
## 安装cpu版本mlc 参考https://llm.mlc.ai/docs/install/mlc_llm.html # Install MLC LLM Python Package

pip install --pre --force-reinstall mlc-ai-nightly-cpu mlc-llm-nightly-cpu -f https://mlc.ai/wheels

## 安装huggingface_hub 为了下载模型
pip install huggingface_hub
</code></pre><h3 id="42-下载模型">4.2 下载模型</h3>
<h4 id="421-本地可以搞一个代码目录">4.2.1 本地可以搞一个代码目录</h4>
<pre><code>mkdir /home/win/llmworkspace
cd  /home/win/llmworkspace
## 这个目录用来存储模型库
mkdir models 
</code></pre><h4 id="422-直接使用gpt生成一个下载脚本">4.2.2 直接使用GPT生成一个下载脚本</h4>
<pre><code>from huggingface_hub import snapshot_download
import os

def download_model():
    # 设置国内镜像
    #os.environ['HF_ENDPOINT'] = &quot;https://hf-mirror.com&quot;

    # 设置本地保存路径
    local_dir = &quot;/home/win/llmworkspace/models/Qwen/Qwen2.5-3B-instruct&quot;

    # 创建保存目录
    os.makedirs(local_dir, exist_ok=True)

    try:
        # 下载模型
        snapshot_download(
            repo_id=&quot;Qwen/Qwen2.5-3B-instruct&quot;,
            local_dir=local_dir,
            token=&quot;*********&quot;  # 替换为你的 HuggingFace token
        )
        print(f&quot;模型已成功下载到: {local_dir}&quot;)

    except Exception as e:
        print(f&quot;下载过程中出现错误: {str(e)}&quot;)

if __name__ == &quot;__main__&quot;:
    download_model()
</code></pre><h4 id="423-下载模型">4.2.3 下载模型</h4>
<pre><code># conda mlc_pre环境 执行下载脚本
python downloadmodel.py

## 下载完成后输出如下信息，Qwen2.5-3B-instruct 大概5.8G
模型已成功下载到: /home/win/llmworkspace/models/Qwen/Qwen2.5-3B-instruct███████████████████████████████████████████████████████████████████████████████████| [04:26&lt;00:00, 12.0MB/s]
</code></pre><h2 id="43-转换模型为mlc格式">4.3 转换模型为MLC格式</h2>
<h4 id="431-直接gpt生成一个转换脚本convermode_pcpy">4.3.1 直接GPT生成一个转换脚本convermode_pc.py</h4>
<pre><code>import os

# 定义变量
MODEL_NAME = &quot;Qwen2.5-3B-Instruct&quot;
QUANTIZATION = &quot;q4f16_1&quot; 
MODEL_PATH = &quot;/home/win/llmworkspace/models/Qwen&quot;
OUT_DIR = &quot;/home/win/llmworkspace/models/ConvertMode&quot;

def convert_model():
    try:
        # 1. 转换模型权重
        print(&quot;开始转换模型权重...&quot;)
        os.system(f&quot;python -m mlc_llm convert_weight {MODEL_PATH}/{MODEL_NAME} &quot;
                 f&quot;--source-format huggingface-safetensor &quot;  # 或 huggingface
                 f&quot;--quantization {QUANTIZATION} &quot;
                 f&quot;-o {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}/&quot;)

        # 2. 生成配置文件
        print(&quot;生成配置文件...&quot;)
        os.system(f&quot;&quot;&quot;python -m mlc_llm gen_config {MODEL_PATH}/{MODEL_NAME} \
            --quantization {QUANTIZATION} \
            --conv-template qwen2 \
            --context-window-size 2048 \
            --prefill-chunk-size 512 \
            -o {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}/&quot;&quot;&quot;)

        print(&quot;转换完成！&quot;)
        print(f&quot;输出文件位置: {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}-android/{MODEL_NAME}-{QUANTIZATION}&quot;)

    except Exception as e:
        print(f&quot;转换过程中出现错误: {str(e)}&quot;)

if __name__ == &quot;__main__&quot;:
    convert_model()
</code></pre><h4 id="432-conda-mlc_pre环境-执行转换脚本">4.3.2 conda mlc_pre环境 执行转换脚本</h4>
<pre><code>python convertmode_pc.py
</code></pre><p>使用q4f16_1转换后模型大概1.7G</p>
<h2 id="44-pc部署运行">4.4 PC部署运行</h2>
<p>pc上面比较简单，conda mlc_pre环境 运行</p>
<pre><code>mlc-llm chat /home/win/llmworkspace/models/ConvertMode/Qwen2.5-3B-Instruct-q4f16_1

### 下面就可以聊天了，这是我的聊天记录

You can use the following special commands:
  /help               print the special commands
  /exit               quit the cli
  /stats              print out stats of last request (token/sec)
  /metrics            print out full engine metrics
  /reset              restart a fresh chat
  /set [overrides]    override settings in the generation config. For example,
                      `/set temperature=0.5;top_p=0.8;seed=23;max_tokens=100;stop=str1,str2`
                      Note: Separate stop words in the `stop` option with commas (,).
  Multi-line input: Use escape+enter to start a new line.

&gt;&gt;&gt; hello
Hello! How can I assist you today?
&gt;&gt;&gt; 小明比小红年龄大，小红比小华年龄大。谁是年龄最小的？
根据题目中的信息，我们知道：

1. 小明比小红年龄大。
2. 小红比小华年龄大。

从这两个信息中可以得出小明年龄最大，小华年龄最小。所以，年龄最小的是小华。
&gt;&gt;&gt; 如果一个气球被放在太阳下暴晒，会发生什么？为什么？
当气球被放在太阳下暴晒时，气球内部的空气会因为温度升高而膨胀。具体来说，以下几个因素会导致气球发生的变化：

1. **温度升高**：太阳光直接照射在气球上，使气球表面温度升高。
2. **热胀冷缩**：温度升高会导致气球内部气体分子的平均动能增加，分子之间的平均距离增加，从而使得气球内部气体膨胀。
3. **气球材料变化**：气球材料（通常是橡胶或塑料）在高温下可能会发生变形或变软，这取决于材料的热稳定性。

最终结果是，气球内部的压力增加，而气球外部的压力相对较小（因为外部环境温度较低）。这种压力差会导致气球内部的压力超过气球材料所能承受的最大压力，从而使气球膨胀，甚至可能破裂。

所以，气球被太阳暴晒时，内部气体膨胀，气球可能会因为压力增大而破裂。
&gt;&gt;&gt;

好的，如果气球被放在太阳下暴晒，可能会发生气球破裂的情况。这是因为气球内部的气体在高温下膨胀，而气球材料在高温下可能变得不那么牢固，导致气球破裂。具体情况取决于气球的材料和温度条件。

如果你有其他问题或需要进一步的信息，请告诉我！
&gt;&gt;&gt;
当然，我可以继续帮助你！如果你有关于气球在太阳下暴晒的问题或者其他任何问题，请随时告诉我。你可以具体描述一下你关心的情况，我会尽力解答。
&gt;&gt;&gt;
好的，有什么具体问题或需要进一步的信息，请告诉我！我会尽力帮助你。
</code></pre><h2 id="45-android部署">4.5 Android部署</h2>
<p>这是官方文档说明可以参考
<a href="https://llm.mlc.ai/docs/deploy/android.html">https://llm.mlc.ai/docs/deploy/android.html</a></p>
<h4 id="451-转换为android格式">4.5.1 转换为Android格式</h4>
<p>之前的convertmode_pc 脚本修改一下,另外我用的pixel3a手机，不支持gpu运行，性能太差，用1.5参数的模型（Qwen2.5-1.5B-Instruct）下载这个模型用前面的下载脚本就行。</p>
<pre><code>import os

# 定义变量
MODEL_NAME = &quot;Qwen2.5-1.5B-Instruct&quot;
QUANTIZATION = &quot;q4f16_1&quot;
MODEL_PATH = &quot;/home/win/llmworkspace/models/Qwen&quot;
OUT_DIR = &quot;/home/win/llmworkspace/models/Android&quot;

def convert_model():
    try:
        # 1. 转换模型权重
        print(&quot;开始转换模型权重...&quot;)
        os.system(f&quot;python -m mlc_llm convert_weight {MODEL_PATH}/{MODEL_NAME} &quot;
                 f&quot;--source-format huggingface-safetensor &quot;  # 或 huggingface
                 f&quot;--quantization {QUANTIZATION} &quot;
                 f&quot;-o {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}-android/&quot;)

        # 2. 生成配置文件
        print(&quot;生成配置文件...&quot;)
        os.system(f&quot;&quot;&quot;python -m mlc_llm gen_config {MODEL_PATH}/{MODEL_NAME} \
            --quantization {QUANTIZATION} \
            --conv-template qwen2 \
            --context-window-size 2048 \
            --prefill-chunk-size 512 \
            -o {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}-android/&quot;&quot;&quot;)

        # 3. 编译为安卓格式
        print(&quot;编译为安卓格式...&quot;)
        os.system(f&quot;&quot;&quot;python -m mlc_llm compile \
            {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}-android/mlc-chat-config.json \
            --device android \
            -o {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}-android/{MODEL_NAME}-{QUANTIZATION}-android.tar&quot;&quot;&quot;)

        print(&quot;转换完成！&quot;)
        print(f&quot;输出文件位置: {OUT_DIR}/{MODEL_NAME}-{QUANTIZATION}-android/{MODEL_NAME}-{QUANTIZATION}-android.tar&quot;)

    except Exception as e:
        print(f&quot;转换过程中出现错误: {str(e)}&quot;)

if __name__ == &quot;__main__&quot;:
    convert_model()
</code></pre><h4 id="452-mlc-llm-源码下载">4.5.2 mlc-llm 源码下载</h4>
<p>注意依赖前置环境rust和ndk</p>
<pre><code># 下载mlc-llm 源码
git clone https://github.com/mlc-ai/mlc-llm.git

cd mlc-llm

git submodule update --init --recursive
cd android/MLCChat

mkdir dist
cd dist
mkdir lib
cd lib
## copy 转换后的模型到/dist/lib

cp /home/win/llmworkspace/models/Android/Qwen2.5-1.5B-Instruct-q4f16_1-android/Qwen2.5-1.5B-Instruct-q4f16_1-android.tar /home/win/llmworkspace/mlc-llm/android/MLCChat/dist/lib/

备份原有的mlc-package-config
mv /home/win/llmworkspace/mlc-llm/android/MLCChat/mlc-package-config.json /home/win/llmworkspace/mlc-llm/android/MLCChat/mlc-package-config.json.back

</code></pre><h4 id="453-自定义mlc-package-config文件">4.5.3 自定义mlc-package-config文件</h4>
<p>修改mlc-package-config 为自己的模型。</p>
<pre><code>vi /home/win/llmworkspace/mlc-llm/android/MLCChat/mlc-package-config.json

{
  &quot;device&quot;: &quot;android&quot;,
  &quot;model_list&quot;: [
    {
      &quot;model&quot;: &quot;/home/win/llmworkspace/models/Android/Qwen2.5-1.5B-Instruct-q4f16_1-android&quot;,
      &quot;bundle_weight&quot;: true,
      &quot;model_id&quot;: &quot;qwen2_q4f16_1&quot;,
      &quot;model_lib&quot;: &quot;qwen2_q4f16_1&quot;,
      &quot;estimated_vram_bytes&quot;: 1630522470,
      &quot;overrides&quot;: {
        &quot;context_window_size&quot;: 512,
        &quot;prefill_chunk_size&quot;: 128
      }
    }
  ],
  &quot;model_lib_path_for_prepare_libs&quot;: {
    &quot;qwen2_q4f16_1&quot;: &quot;./dist/lib/Qwen2.5-1.5B-Instruct-q4f16_1-android.tar&quot;
  }
}

</code></pre><h4 id="454-编译so库">4.5.4 编译so库</h4>
<pre><code>conda mlc_pre环境 编译mlc 源码

cd /home/win/llmworkspace/mlc-llm/android/MLCChat/

## 编译
mlc_llm package

## /home/win/llmworkspace/mlc-llm/android/MLCChat/这个目录下输出
dist
└── lib
    └── bundle
        ├── qwen2.5-1.5b-q4f16_1
    └── mlc4j
        ├── build.gradle
        ├── output
        │   ├── arm64-v8a
        │   │   └── libtvm4j_runtime_packed.so
        │   └── tvm4j_core.jar
        └── src
            ├── cpp
            │   └── tvm_runtime.h
            └── main
                ├── AndroidManifest.xml
                ├── assets
                │   └── mlc-app-config.json
                └── java
                    └── ...
</code></pre><h4 id="455-编译和运行apk并push权重参数">4.5.5 编译和运行apk并push权重参数</h4>
<pre><code>我是把上面的/home/win/llmworkspace/mlc-llm/android/MLCChat/ copy到一个新的android studio 工程单独编译的。

# android studio 编译apk

#conda mlc_pre 环境执行脚本

cd /home/win/project/demo/MLCChat/

python bundle_weight.py --apk-path 
/home/win/project/demo/MLCChat/app/build/outputs/apk/debug/app-debug.apk

</code></pre><p>其实bundle_weight.py 这个脚本干了3件事,也可以手动执行</p>
<pre><code>1.adb install xxx.apk
2.adb push //home/win/project/demo/MLCChat/dist/bundle/qwen2.5-1.5b-q4f16_1 /data/local/tmp/qwen2.5-1.5b-q4f16_1
3.mv /data/local/tmp/qwen2.5-1.5b-q4f16_1 /storage/emulated/0/Android/data/ai.mlc.mlcchat/files/
</code></pre><h4 id="开始对话">开始对话</h4>
<p>
  <img src="/img/mlc-llm-android%e8%bf%90%e8%a1%8c%e7%bb%93%e6%9e%9c.jpg" alt="android运行结果">

</p>
<ul>
<li>评价
pixel3a 手机，纯cpu运行，经常内存溢出，可以用更好的手机</li>
</ul>
<h2 id="引用">引用</h2>
<ul>
<li>
<p><a href="https://llm.mlc.ai/docs/install/mlc_llm.html">Install MLC LLM Python Package</a></p>
</li>
<li>
<p><a href="https://llm.mlc.ai/docs/deploy/android.html">https://llm.mlc.ai/docs/deploy/android.html</a></p>
</li>
<li>
<p><a href="https://medium.com/google-developer-experts/ml-story-mobilellama3-run-llama3-locally-on-mobile-36182fed3889">[ML Story] MobileLlama3: Run Llama3 locally on mobile</a></p>
</li>
<li>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/">清华大学开源软件镜像站</a></p>
</li>
<li>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/</a></p>
</li>
</ul>


                
                
<div class="entry-shang text-center">
    
	    <p>「真诚赞赏，手留余香」</p>
	
	<button class="zs show-zs btn btn-bred">赞赏支持</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
	<div class="zs-modal-head">
		<button type="button" class="close">×</button>
		<span class="author"><a href="https://ywqyunshan.github.io/"><img src="/img/favicon.png" />FQ Blog</a></span>
        
	        <p class="tip"><i></i><span>真诚赞赏，手留余香</span></p>
		
 
	</div>
	<div class="zs-modal-body">
		<div class="zs-modal-btns">
			<button class="btn btn-blink" data-num="2">2元</button>
			<button class="btn btn-blink" data-num="5">5元</button>
			<button class="btn btn-blink" data-num="10">10元</button>
			<button class="btn btn-blink" data-num="50">50元</button>
			<button class="btn btn-blink" data-num="100">100元</button>
			<button class="btn btn-blink" data-num="1">任意金额</button>
		</div>
		<div class="zs-modal-pay">
			<button class="btn btn-bred" id="pay-text">2元</button>
			<p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
			<img src="/img/reward/wechat-2.png"  id="pay-image"/>
		</div>
	</div>
	<div class="zs-modal-footer">
		<label><input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked"><span ><span class="zs-wechat"><img src="/img/reward/wechat-btn.png"/></span></label>
		<label><input type="radio" name="zs-type" value="alipay" class="zs-type" class="zs-alipay"><img src="/img/reward/alipay-btn.png"/></span></label>
	</div>
</div>
<script type="text/javascript" src="/js/reward.js"></script>

                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2024/08/21/" data-toggle="tooltip" data-placement="top" title="Mesa3d环境编译">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2025/11/01/" data-toggle="tooltip" data-placement="top" title="谈谈整车电子电器架构和全域操作系统">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>
                

                



            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        <a href="/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98" title="工程实战">
                            工程实战
                        </a>
                        
                        
                        
                        <a href="/tags/%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA" title="系统理论">
                            系统理论
                        </a>
                        
                        
                    </div>
                </section>
                

                
                
                <section>
                    <hr>
                    <h5>FRIENDS</h5>
                    <ul class="list-inline">
                        
                        <li><a target="_blank" href="https://www.jianshu.com/u/e9dc7e422257">iigeoywq的博客</a></li>
                        
                        <li><a target="_blank" href="https://www.jianshu.com/u/978b4a822bc3">button的博客</a></li>
                        
                    </ul>
                </section>
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    <li>
                        <a href="mailto:youremail@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    
                    

		            
                    
                    <li>
                        <a target="_blank" href="/your%20wechat%20qr%20code%20image">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-weixin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    <li>
                        <a target="_blank" href="https://github.com/yourgithub">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/yourlinkedinid">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    <li>
                        <a target="_blank" href="https://stackoverflow.com/users/yourstackoverflowid">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-stack-overflow fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
            
            
            
           
                   <li>
                       <a href='' rel="alternate" type="application/rss+xml" title="FQ Blog" >
                           <span class="fa-stack fa-lg">
                               <i class="fas fa-circle fa-stack-2x"></i>
                               <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
            
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; FQ Blog 2025
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
